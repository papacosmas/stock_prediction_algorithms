---
title: 'Capstone HarvardX - Project: Algorithms Comparison for Financial Shares Forecasting'
author: "Papacosmas Nicholas"
date: "`r format(Sys.time(), '%B %Y')`"
output:
  pdf_document:
fig_width: 14
fig_height: 10
latex_engine: xelatex
fontsize: 12pt
number_sections: yes
toc: yes
toc_depth: 3
df_print: kable
fig_crop: false
urlcolor: blue
linkcolor: red
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

```{r echo=FALSE,warning=FALSE}
 library(knitr)
  opts_chunk$set(fig.path='figure/pics-', 
                 cache.path='cache/pics-', 
                 fig.align='center',
                 external=TRUE,
                 echo=TRUE,
                 warning=FALSE,
                 comment = NA)

```
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, comment = NA,echo=FALSE, warning=FALSE, message=FALSE)
```
\newpage
 
#01.Abstract
The financial markets contain a plethora of statistical patterns. The
behavior of those patterns is similar with the behavior of the natural phenomena
patterns. That means that both are affected by unknown and unstable variables. 
Which leads to high unpredictability and volatility. That makes almost impossible
to forecast future behavior.

*As Burton Malkiel, who argues in his 1973 book, "A Random Walk Down Wall Street,"*

Nevertheles, one forecasting methodology is: To use the past performance of markets 
as a predictor for the future. That can be achieved by observing the changes of 
small seasonal intervals, when the time series is stationary. 

#02.Introduction 
The purpose of this project is to analyze 3 different algorithms for the financial
forecasting of Daimler share. Disclaimer I am working on Advanced Analytics of 
Daimler AG. This analysis is for educational purposes and not for financial advising.

#03.Methodology
We will use Daimler historical share market datasets (from 2010). For forecasting future 
values. On those nature of forecasting we assume that some patterns of our sets 
have carriage on future short linear interims. The same approach is applied on 
the weather forecasting.

We will apply mathematical technical indicators in our datasets on the below domains:  
-**Support & resistance**  
-**Trend**  
-**Momentum**  
-**Volume**  
-**Volatility**  

Some of them are the:  
-**Moving average convergence/divergence**  
-**Relative strength index**  
-**Stochastic oscillator**  
-**Ease of movement**  
-**Larry Williams oscilator. Etc.**  
  
The 3 algorithms that will we compare to predict the Daimler financial share 
behavior are:

-(LASSO) Least Absolute Shrinkage and Selection Operator. This method is based
on a linear regression model is proposed as a novel method to predict financial 
market behavior  
  
-Deep Learning (Long Short Term Memory Neural Network of linear   
stack densely connected layers.  

-And eXtreme Gradient Boosting  

Apreciation to the colleagues from:  
*Cornell University (arXiv:1512.04916v3) [q-fin.CP] (Ruoxua, 2016)*  

*Cornell University Social and Information Networks (cs.SI); Computational*   
*Finance (q-fin.CP) (Jichang Zhao,2019)*  

#04.Disclaimer
*This article is intended for academic and educational purposes and is not an *
*investment recommendation. The information that we provide or should not be a *
*substitute for advice from an investment professional. The models discussed in* 
*this paper do not reflect the investment performance. A decision to invest in* 
*any product or strategy should not be based on the information or conclusions*
*contained herein. This is neither an offer to sell / buy nor a solicitation for* 
*an offer to buy interests in securities.*

```{r, echo=FALSE,  warning=FALSE, message=FALSE }
library(dplyr)
library(ggthemes)
options(kableExtra.latex.load_packages = FALSE)
  require(kableExtra)
library(dynlm)
library(ggplot2)
library(tidyr)
library(tseries)
library(TSA)
library(forecast)
library(gridExtra)
library(corrplot)
library(data.table)
library(quantmod)
library(plotly)
library(GGally)
library(data.table)
library(kableExtra)
library(plotly)
library(caret)
library(xgboost)
library(dplyr)
library(lubridate)
library(DT)
library(xts)
library(e1071)
library(doParallel)
library(keras)
library(tensorflow)
library(knitr)

load("daimler_stock_env.RData")

```

```{r, echo=FALSE,warning=FALSE, message=FALSE,eval=FALSE}

start <- as.Date("2010-05-01")
end <- as.Date("2019-05-01")

require(quantmod)
getSymbols("DDAIF", from = start, to = end)
```
#05.Data Observation
**We will use as data sets the Daimler AG Symbol (DDAIF)**

Display of the 6 first entries of our dataset
```{r, echo=FALSE,warning=FALSE, message=FALSE}
apply(DDAIF, 2, function(x) any(is.na(x)))

head(DDAIF)
glimpse(DDAIF)
```
```{r, echo=FALSE,warning=FALSE, message=FALSE,eval=FALSE}

DDAIF_log_returns <- DDAIF%>%Ad()%>%dailyReturn(type='log')
```

\newpage

Chart series technical analysis graph  
```{r, echo=FALSE,  warning=FALSE, message=FALSE }
DDAIF %>% Ad() %>%chartSeries()
```
\newpage

Bollinger Bands, and Moving  Average Convergence Divergence
```{r, echo=FALSE,  warning=FALSE, message=FALSE }

DDAIF %>% chartSeries(TA='addBBands();addVo();addMACD()',subset='2018')

```

\newpage

Graph with opening Prices since 2010 pro semester
```{r, echo=FALSE, warning=FALSE, message=FALSE }

require(ggthemes)
require(ggplot2)
require(plotly)
ggplot(DDAIF, aes(x = index(DDAIF), y = DDAIF[,1])) + geom_line(color = "yellow") + 
  ggtitle("Daimler stock Opening Prices") + xlab("Date") + ylab("Opening Prices") + 
  scale_x_date(date_labels = "%b %y", date_breaks = "6 months")+
  theme_solarized(light=FALSE)+
  theme(axis.text=element_text(size=10,angle = 90),plot.title = element_text(size = 11, color = "yellow", hjust = 0.5))
```
\newpage
Graph with the Adjusted Closing Prices
```{r, echo=FALSE, warning=FALSE, message=FALSE }

ggplot(DDAIF, aes(x = index(DDAIF), y = DDAIF[,6])) + geom_line(color = "yellow") + 
  ggtitle("Daimler Adjusted Closing Prices") + xlab("Date") + ylab("Adjusted Closing Prices") + 
  scale_x_date(date_labels = "%b %y", date_breaks = "6 months") +
  theme_solarized(light=FALSE)+
  theme(axis.text=element_text(size=10,angle = 90),plot.title = element_text(size = 11, color = "yellow", hjust = 0.5))
```
\newpage

Plot of an An open-high-low-close chart 
```{r, echo=FALSE, warning=FALSE, message=FALSE }

require(gridExtra)
options(repr.plot.width=10, repr.plot.height=10) 
popen = ggplot(DDAIF, aes(DDAIF.Open)) + geom_histogram(bins = 50, aes(y = ..density..), col = "yellow", fill = "yellow", alpha = 0.2) + geom_density()
phigh = ggplot(DDAIF, aes(DDAIF.High)) + geom_histogram(bins = 50, aes(y = ..density..), col = "blue", fill = "red", alpha = 0.2) + geom_density()
plow = ggplot(DDAIF, aes(DDAIF.Low)) + geom_histogram(bins = 50, aes(y = ..density..), col = "red", fill = "red", alpha = 0.2) + geom_density()
pclose= ggplot(DDAIF, aes(DDAIF.Close)) + geom_histogram(bins = 50, aes(y = ..density..), col = "black", fill = "red", alpha = 0.2) + geom_density()
grid.arrange(popen,phigh,plow,pclose, nrow=2,ncol=2)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

#We create a response variable. To predicting future days price, we apply 
# lag function in the price change

# We Calculate the various moving averages (MA) of a series for volume.
#For the past 10, 20 , 60 days
require(TTR)
DDAIF$Avg_volume_10  <- SMA(DDAIF$DDAIF.Volume, n = 10)
DDAIF$Avg_volume_20  <- SMA(DDAIF$DDAIF.Volume, n = 20)
DDAIF$Avg_volume_60  <- SMA(DDAIF$DDAIF.Volume, n = 60)

# We calculate the % of the average volume of the above days
DDAIF$Volume_perc_avg_10 <- (DDAIF$DDAIF.Volume/DDAIF$Avg_vol_10)*100
DDAIF$Volume_perc_avg_20 <- (DDAIF$DDAIF.Volume/DDAIF$Avg_vol_20)*100
DDAIF$Volume_perc_avg_60 <- (DDAIF$DDAIF.Volume/DDAIF$Avg_vol_60)*100

# We calculate the range between high and low 
DDAIF$Range <- DDAIF$DDAIF.High - DDAIF$DDAIF.Low 

# % change of closing price. 
DDAIF$perc_change_closing <- (DDAIF$DDAIF.Close - lag(DDAIF$DDAIF.Close))/lag(DDAIF$DDAIF.Close) * 100

# Range between  prior days closing price and todays closing price
DDAIF$change_from_yest <- DDAIF$DDAIF.Close - lag(DDAIF$DDAIF.Close)

# We Calculate again the various moving averages (MA) for range now .
#For the past 10, 20 , 60 days
DDAIF$moving_avg_10 <- SMA(DDAIF$Range, n = 10)
DDAIF$moving_avg_20 <- SMA(DDAIF$Range, n = 20)
DDAIF$moving_avg_60 <- SMA(DDAIF$Range, n = 60)

# We calculate the % of the average range of the above days
DDAIF$perc_moving_avg_10  <- (DDAIF$Range/DDAIF$moving_avg_10) * 100
DDAIF$perc_moving_avg_20 <- (DDAIF$Range/DDAIF$moving_avg_20) * 100
DDAIF$perc_moving_avg_60 <- (DDAIF$Range/DDAIF$moving_avg_60) * 100

# The tot amount of money traded multiplied by the volume (in dollars)
DDAIF$cash_tradet <- DDAIF$DDAIF.Close*DDAIF$DDAIF.Volume

# The average volume of cash trated for the same periods as above
DDAIF$avg_cash_trated_10 <- SMA(DDAIF$cash_tradet, n = 10)
DDAIF$avg_cash_trated_20 <- SMA(DDAIF$cash_tradet, n = 20)
DDAIF$avg_cash_trated_60 <- SMA(DDAIF$cash_tradet, n = 60)

# The % of the avgo volume today.
DDAIF$Avg_Dollar_volume_pct_10 <- (DDAIF$cash_tradet/DDAIF$avg_cash_trated_10) * 100
DDAIF$Avg_Dollar_volume_pct_20 <- (DDAIF$cash_tradet/DDAIF$avg_cash_trated_20) * 100
DDAIF$Avg_Dollar_volume_pct_60 <- (DDAIF$cash_tradet/DDAIF$avg_cash_trated_60) * 100
```

```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

# Todays open vs Yesterdays Close. 
require(data.table)
require(dplyr)

DDAIF$nightgap <- DDAIF$DDAIF.Open - lag(DDAIF$DDAIF.Close)

# The Gap % win or loss from yesterday closing prices
DDAIF$night_gap_perc <- (DDAIF$DDAIF.Open - lag(DDAIF$DDAIF.Close))/lag(DDAIF$DDAIF.Close) * 100
DDAIF$perc_range_previous = abs((DDAIF$DDAIF.Close - DDAIF$DDAIF.Open)/(DDAIF$DDAIF.High-DDAIF$DDAIF.Low)*100)
DDAIF$perc_range_atpr = (DDAIF$Range/DDAIF$DDAIF.Close)*100
DDAIF$perc_range_williams = (DDAIF$DDAIF.High-DDAIF$DDAIF.Close)/(DDAIF$DDAIF.High-DDAIF$DDAIF.Low)*100
# Compute range for 1 Month
require(zoo)
one_month_range_perc <- rollapply(DDAIF$DDAIF.High, 20, max) - rollapply(DDAIF$DDAIF.Low, 20, max)

DDAIF$one_month_range_perc = (DDAIF$DDAIF.Close- DDAIF$DDAIF.Low)/one_month_range_perc*100
gc()#clean RAM
```

```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

# Moving averages smooth the price data to form a trend following indicator. They
require(TTR)
DDAIF$EMA10 <- EMA(DDAIF$DDAIF.Low, n = 10)
DDAIF$EMA20 <- EMA(DDAIF$DDAIF.Low, n = 20)
#Weighted Moving Average
DDAIF$EMA60 <- EMA(DDAIF$DDAIF.Low, n = 60)
#Double Exponential Moving Average is a measure of a security's trending average
DDAIF$WMA10 <- WMA(DDAIF$DDAIF.Low, n = 10)
# The EVWMA uses the volume to declare the period of the MA.
DDAIF$EVWMA10 <- EVWMA(DDAIF$DDAIF.Low, DDAIF$DDAIF.Volume)
#Zero Lag Exponential Moving Average (ZLEMA) As is the case with the double 
DDAIF$ZLEMA10 <- ZLEMA(DDAIF$DDAIF.Low, n = 10)
#Volume weighted average price (VWAP) and moving volume weighted average price 
DDAIF$VWAP10 <- VWAP(DDAIF$DDAIF.Low, DDAIF$DDAIF.Volume)
#The Hull Moving Average (HMA), developed by Alan Hull, is an extremely fast 
DDAIF$HMA10 <- HMA(DDAIF$DDAIF.Low, n = 20)
#The ALMA moving average uses curve of the Normal (Gauss) distribution which 
DDAIF$ALMA10 <- ALMA(DDAIF$DDAIF.Low, n = 9, offset = 0.85, sigma = 6)
#DDAIF <- DDAIF[complete.cases(DDAIF), ]
write.csv(DDAIF, file = "DDAIF_with_TI.csv", row.names = F)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE}
# Augmented Dickey-Fuller test AND correlation test          
require(tseries)
#From package tseries we use the () adf.test- Computes the Augmented Dickey-Fuller 
#test for the null that x has a unit root. 
adf.test(DDAIF$DDAIF.Adjusted)
#The result shows that we need to move stationarity
require(forecast)
require(xts)
require(e1071)
require(doParallel)
require(dynlm)
require(caret)
require(dynlm)

DDAIF_lm <- na.omit(DDAIF)#we handle missing values
set.seed(123)#algorithm for reproducability
X <- DDAIF_lm[,-6]
y <- DDAIF_lm[,6]

# We scale the variables in order to run the models
X.scaled <- scale(X)
gc()#clean RAM

# We merge them back 
DDAIF_lm <- cbind(X.scaled, y)

#create index
numerical_Vars <- which(sapply(DDAIF_lm, is.numeric)) 

#save the vector
numerical_VarNames <- names(numerical_Vars)
cat('They exist', length(numerical_Vars), 'numerical variables.\n')

sum_numVar <- DDAIF_lm[, numerical_Vars]

#We calculate the correlations of all numerical variables
correl_numVar <- cor(sum_numVar, use="pairwise.complete.obs") 

#We order of the decreasing correlations vs sales price
correl_sorted <- as.matrix(sort(correl_numVar[,'DDAIF.Adjusted'], decreasing = TRUE))

#We choose only the high corellated
Correl_High <- names(which(apply(correl_sorted, 1, function(x) abs(x)>0.5)))
correl_numVar <- correl_numVar[Correl_High, Correl_High]
```
```{r,dpi= 96,out.width="200%", echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

require(corrplot)

corrplot.mixed(correl_numVar, tl.col="black", tl.pos = "lt", number.cex=0.5)
gc()#clean RAM
```

```{r, echo=FALSE,eval=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

# We remove the highly correlated variables to avoid overfitting of models           
del <- cor(DDAIF_lm)
del[upper.tri(del)] <- 0
diag(del) <- 0

DDAIF_lm <- DDAIF_lm[,!apply(del,2,function(x) any(x > 0.90))]

#------------------------------------------
# We create our Train and Test Datasets
#------------------------------------------
# For next day forecast n = days_forecast + 1. If you want more days change the 
#+1
days_to_forecast = 7
n = days_to_forecast + 1
X_train = DDAIF_lm[1:(nrow(DDAIF_lm)-(n-1)),-17]
# Our dependent var: Is the price adj
y_train = DDAIF_lm[n:nrow(DDAIF_lm),17]
X_test = DDAIF_lm[((nrow(DDAIF_lm)-(n-2)):nrow(DDAIF_lm)),-17]

require(quantmod)
#We create the validation test of the real prices of the next 7 days
# Adapt dates according to your n days of forecast
DDAIF2 = getSymbols('DDAIF', from='2019-04-22', to='2019-05-01',auto.assign = FALSE)

y_test <- as.numeric(DDAIF2$DDAIF.Adjusted)

train <- cbind(X_train,y_train)
#check the number of features
dim(X_train)
dim(X_test)



#------------------------------------------
# KERAS DEEP LEARNING : backend TensorFlow
#------------------------------------------
# We will apply deep learning networks of linear stack densely connected layers
#If you already have installed Keras and tensorflow then skip the below commands
# devtools::install_github("rstudio/keras")
# devtools::install_github("rstudio/tensorflow")
#install_tensorflow()
require(keras)
require(tensorflow)

ker = ncol(X_train)
```
#06.Keras with LSTM
We will apply deep learning networks of linear stack densely connected layers
```{r, echo=TRUE,eval=FALSE, warning=FALSE, message=FALSE,eval=FALSE }

keras_model <- keras_model_sequential() 

keras_model %>% 
  #We ddd a densely-connected NN layer to an output
  #ReLU (Rectified Linear Unit) Activation Function
  layer_dense(units = 60, activation = 'relu', input_shape = ker) %>% 
  layer_dropout(rate = 0.2) %>% #We apply dropout  to prevent overfitting
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'linear')
```
```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE }
str(keras_model)
```
```{r, echo=TRUE,eval=FALSE, warning=FALSE, message=FALSE }

keras_model %>% compile(
  optimizer = 'rmsprop',
  loss = 'mse',
  metrics = 'mse')
```
We train the NN model 
```{r, echo=TRUE, warning=FALSE,eval=FALSE, message=FALSE }
keras_history <- keras_model %>% fit(X_train, y_train, epochs=200,
                                     batch_size=28, validation_split = 0.1,
                                     callbacks = callback_tensorboard("logs/run_a"))
```

```{r, echo=TRUE,eval=FALSE, warning=FALSE, message=FALSE }
keras_pred <- keras_model %>% predict(X_test, batch_size = 28)
```

Plot of Keras Model History
```{r, echo=FALSE, warning=FALSE, message=FALSE }
plot(keras_history)
```
\newpage

Plot of Keras Model Predictions vs Actuals
```{r, echo=FALSE, warning=FALSE, message=FALSE }
#------------------------------------------
# Step 3: We plot the predictions 
#------------------------------------------
require(ggplot2)
require(ggthemes)
require(plotly)

real_VS_pred <- data.frame(keras_pred,y_test)
colnames(real_VS_pred) <- c("KERAS PRED","REAL PRICES")    

ggplot(real_VS_pred, aes(date)) + 
  geom_line(aes(y = keras_pred, colour = "keras_pred"))+
  geom_line(aes(y = y_test, colour = "real_prices"))+
  geom_point(aes(y = keras_pred, colour = "keras_pred"), size=2) +
  geom_point(aes(y = y_test, colour = "real_prices"), size=2) +
  labs(title = "Keras (Predicted vs Actual)",x = "Date",y = "Daimler Share Price in $")+
  theme_solarized(light=FALSE) 


#We display Pred vs Actual of all models   
require(kableExtra)
kable(real_VS_pred) %>%
  kable_styling(bootstrap_options = "bordered" , full_width = F , position = "center") %>%
  column_spec(1,bold = T ,color = "red" )
```


#07.Lasso regression model   
With caret package we will apply cross validation in order to find the optimal 
hyperparameters

```{r, echo=TRUE,eval=FALSE, warning=FALSE, message=FALSE }

require(caret)

train$DDAIF.Adjusted <- as.numeric(train$DDAIF.Adjusted)
set.seed(123)#algorithm for reproducability
trainControl <-trainControl(method="cv", number=5)
lassoGrid <- expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0005))

lassomod <- train(DDAIF.Adjusted ~., data = na.omit(train), method='glmnet', trControl= trainControl, 
                  tuneGrid=lassoGrid) 
#we display the optimal alpha and lambda penalties
lassomod$bestTune

#We display the root mean squared error
min(lassomod$results$RMSE)

#From caret package we use () varImp. Is a generic method for calculating 
#variable importance for objects produced by train and method specific methods
lasso_VarImp <- varImp(lassomod,scale=F)
lasso_Importance <- lasso_VarImp$importance
vars_Selected <- length(which(lasso_Importance$Overall!=0))
vars_NotSelected <- length(which(lasso_Importance$Overall==0))
```
Display of the Laso Regression vars penalty
```{r, echo=FALSE, warning=FALSE, message=FALSE }
cat('The Lasso regression used', vars_Selected, 'variables, and did not used', vars_NotSelected, 'variables.\n')
```
```{r, echo=TRUE,eval=FALSE, warning=FALSE, message=FALSE }

#Run the prediction for the next 7 days
LassoPred <- predict(lassomod, X_test)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE }

# Display the prediction for the next 7 days
real_VS_pred <- data.frame(keras_pred,LassoPred,y_test)
colnames(real_VS_pred) <- c("KERAS PRED","LASSO PRED","REAL PRICES")    
```
\newpage
Plot of Keras and Lasso regression:Predicted vs Actual Prices
```{r, echo=FALSE, warning=FALSE, message=FALSE }
require(ggplot2)
require(GGally)
require(plotly)

ggplot(real_VS_pred, aes(date)) + 
  geom_line(aes(y = keras_pred, colour = "keras_pred")) + 
  geom_line(aes(y = LassoPred, colour = "LassoPred")) + 
  geom_line(aes(y = y_test, colour = "real_prices"))+
  geom_point(aes(y = keras_pred, colour = "keras_pred"), size=2) +
  geom_point(aes(y = LassoPred, colour = "LassoPred"), size=2) +
  geom_point(aes(y = y_test, colour = "real_prices"), size=2) +
  labs(title = "Keras - Lasso (Predicted vs Actual)",x = "Date",y = "Daimler Share Price in $")+
  theme_solarized(light=FALSE) 

require(kableExtra)
kable(real_VS_pred) %>%
  kable_styling(bootstrap_options = "bordered" , full_width = F , position = "center") %>%
  column_spec(3,bold = T ,color = "red"  )
```
#08.XGBoost model 
We define the parameters that caret will use in the finding of hyperparameters
```{r, echo=TRUE, warning=FALSE,eval=FALSE, message=FALSE }

library(xgboost)
xgb_grid = expand.grid(
  nrounds = 1000,
  eta = c(0.1, 0.05, 0.01),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree=1,
  min_child_weight=c(1, 2, 3, 4 ,5),
  subsample=1)

# With the 5 fold cross validation, caret package can find the optimal hyperparameters
# for our model (takes a lot of time...)
xgb_hyperparam <- train(DDAIF.Adjusted~., data = na.omit(train), method='xgbTree', trControl= trainControl, tuneGrid=xgb_grid) 
```
```{r, echo=FALSE, warning=FALSE,eval=FALSE, message=FALSE }

gc()#clean RAM

#We display the best hyperparameters
xgb_hyperparam$bestTune

#Creation of label
labeltrain <- y_train[!is.na(y_train)]

# For this model we have to tranform our sets into Dmatrix objects
train_dmatrix <- xgb.DMatrix(data = as.matrix(X_train), label= labeltrain)
test_dmatrix <- xgb.DMatrix(data = as.matrix(X_test))

# We apply the best hyperparameters of the cross validation
default_param<-list(
  objective = "reg:linear",
  booster = "gbtree",
  eta=0.1, 
  gamma=0,
  max_depth=3, 
  min_child_weight=2, 
  subsample=1,
  colsample_bytree=1)

#We apply the cross validation function of xgboost to find optimal nrounds
xgb_cv <- xgb.cv( params = default_param, data = train_dmatrix, nrounds = 1000, 
                  nfold = 5, showsd = T, stratified = T, print_every_n = 40, 
                  early_stopping_rounds = 10, maximize = F, verbose = TRUE)

#Best nrounds
xgb_cv        
train the model using the best iteration found by cross validation
xgb_mod <- xgb.train(data = train_dmatrix, params=default_param, nrounds = 829)
xgb_pred <- predict(xgb_mod, test_dmatrix)
xgb_pred
```

#09.Results of all models
```{r, echo=FALSE, warning=FALSE, message=FALSE }

real_VS_pred <- data.frame(keras_pred,LassoPred,xgb_pred,y_test)
colnames(real_VS_pred) <- c("KERAS PRED","LASSO PRED","XGB PRED","REAL PRICES")    

#Results
require(ggplot2)
require(GGally)
require(plotly)
ggplot(real_VS_pred, aes(date)) + 
  geom_line(aes(y = keras_pred, colour = "keras_pred"))+
  geom_line(aes(y = LassoPred, colour = "Lasso_pred")) + 
  geom_line(aes(y = xgb_pred, colour = "xgb_pred"))+
  geom_line(aes(y = y_test, colour = "real_prices"))+
  geom_point(aes(y = keras_pred, colour = "keras_pred"), size=2) +
  geom_point(aes(y = LassoPred, colour = "Lasso_pred"), size=2) +
  geom_point(aes(y = xgb_pred, colour = "xgb_pred"), size=2) +
  geom_point(aes(y = y_test, colour = "real_prices"), size=2) +
  labs(title = "Lasso - XGB (Predicted vs Actual)",x = "Date",y = "Daimler Share Price in $")+
  theme_solarized(light=FALSE) 

require(kableExtra)
kable(real_VS_pred) %>%
  kable_styling(bootstrap_options = "bordered" , full_width = F , position = "center") %>%
  column_spec(4,bold = T ,color = "red"  )
```

#10.Conclusion
Usually the share price daily fluctuation is between 1 - 2 percent in ordinary
time periods. Unfortunately the above models presented high daily fluctuation. 
Regardless from our application of the mathematical technical indicators.
Into our datasets before the training of the models.

Unfortunately currently our models are not adequate to forecast time series 
of markets successfully. 

#11.Proposal
On another paper, i have also created some models to analyze the correlation of 
social media sentiment and Daimler share price. There my models forecasting was
significantly more successful. I would propose to create a model that would
analyze and combine the results of:  
  
**-Social media sentiment**  
**-Economic News**  
**-Market Time Series Analysis**  
  
*Thank you for reading my analysis*  
KR  
Niko  


REFERENCES

[1] Ruoxuan Xiong, Eric P. Nichols, Yuan Shen. Deep Learning Stock Volatility with Google Domestic Trends. Cornell University (arXiv:1512.04916v3) [q-fin.CP] (2016)

[2] Junran Wu, Ke Xu, Jichang Zhao.Online reviews can predict long-term returns of individual stocks
. Cornell University (2019)



